

Word2Vec：通过预测词语与上下文的关系，学习词语的嵌入向量。Word2Vec有两种方法：CBOW（连续词袋模型）和Skip-gram。
GloVe：通过建模词与词之间的共现概率矩阵，学习词嵌入。
FastText：将词语分为子词嵌入，用以处理未见过的词语（Out-of-Vocabulary, OOV）问题。
BERT：通过预训练语言模型，生成上下文敏感的动态词嵌入。

Word2Vec
Word2Vec深入探索了文本内语义关系，其主要有两种训练算法：连续词袋 (CBOW) 和 Skip-gram。
- CBOW：连续词袋模型的目标是根据上下文的词来预测当前词。比如给定上下文“the quick brown”来预测“fox”。它通过一个浅层神经网络，将上下文词的向量进行求和或平均，然后通过一个线性变换和 softmax 函数来预测中心词。这种方法在处理大规模语料时效率较高，能够快速学习到词的表示。
- Skip-gram：Skip-gram 模型则相反，它是根据当前词来预测上下文的词。例如已知“fox”，预测其周围可能出现的词如“the”“quick”“brown”等。Skip-gram 在捕捉词与词之间的语义关系上表现更好，尤其对于低频词，能够学习到更准确的表示。

GloVe
GloVe（单词表示的全局向量）利用全局词 - 词共现统计数据，提供了一种独特的方法来根据语料库中的集体上下文嵌入单词，与 Word2Vec 有所不同 1。它构建了一个词 - 词共现矩阵，通过最小化一个基于共现频率的损失函数来学习词向量。这种方法考虑了整个语料库的统计信息，能够更好地捕捉词之间的语义和句法关系，在一些任务上表现优于 Word2Vec。

FastText
FastText 重点关注处理词汇表外单词的创新方法，将单词分解为更小的单元（n 元语法），增强了单词的表示，尤其在形态丰富的语言中表现出色 1。例如，对于一个单词“apple”，可以将其分解为“ap”“pp”“pl”“le”等 n 元语法。这样即使遇到未在训练语料中出现的单词，也可以通过其 n 元语法的组合来近似表示该单词，提高了模型的泛化能力。

BERT
BERT（Bidirectional Encoder Representations from Transformers）是一种预训练语言模型，它采用了双向 Transformer 架构 4。BERT 通过在大规模语料上进行无监督学习，学习到语言的深层语义表示。它有两种预训练任务：掩码语言模型（Masked Language Model，MLM）和下一句预测（Next Sentence Prediction，NSP）。MLM 随机掩码输入中的一些词，让模型预测这些被掩码的词，从而学习到词的上下文表示；NSP 则判断两个句子是否是连续的，学习句子之间的关系。在实际应用中，BERT 可以通过微调适应各种下游任务，如文本分类、命名实体识别等。

